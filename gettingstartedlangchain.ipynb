{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os  # Provides a way to interact with the operating system (e.g., accessing environment variables)\n",
    "\n",
    "from dotenv import load_dotenv  # Imports the function to load variables from a .env file\n",
    "\n",
    "# Load environment variables from a .env file into the system environment\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f3c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the value of the environment variable 'LANGCHAIN_PROJECT'\n",
    "# Returns None if the variable is not found\n",
    "os.getenv(\"LANCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd7e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47124a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002CD49B21AD0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002CD49B23790> root_client=<openai.OpenAI object at 0x000002CD49B20CD0> root_async_client=<openai.AsyncOpenAI object at 0x000002CD49B22C50> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Agentic AI** refers to artificial intelligence systems that possess a degree of agency, meaning they can act autonomously, make decisions, and pursue goals with minimal human intervention. The concept of agency in AI encapsulates the ability of a system to perceive its environment, interpret data, make informed decisions, and execute actions to achieve specific objectives. This contrasts with more passive or reactive AI systems that primarily respond to direct inputs without engaging in proactive or self-directed behavior.\\n\\n### Key Characteristics of Agentic AI\\n\\n1. **Autonomy:** Agentic AI operates independently, making decisions without continuous human oversight. This autonomy enables the system to function in dynamic environments where pre-defined instructions may not suffice.\\n\\n2. **Goal-Oriented Behavior:** These AI systems are designed to achieve specific objectives. They can set sub-goals, prioritize tasks, and adjust their strategies based on feedback and changing conditions.\\n\\n3. **Perception and Interaction:** Agentic AI can perceive its environment through various sensors or data inputs, interpret this information, and interact with the world or other systems accordingly.\\n\\n4. **Learning and Adaptation:** Such systems often incorporate machine learning algorithms that allow them to learn from experiences, adapt to new situations, and improve their performance over time.\\n\\n5. **Decision-Making Capabilities:** Agentic AI employs sophisticated algorithms to evaluate options, assess risks, and make decisions that align with their programmed goals and any ethical guidelines they might follow.\\n\\n### Examples of Agentic AI\\n\\n- **Autonomous Vehicles:** Self-driving cars are prime examples of agentic AI. They perceive their surroundings using sensors, make real-time decisions to navigate traffic, avoid obstacles, and ensure passenger safety without human intervention.\\n\\n- **Robotic Process Automation (RPA):** Advanced robotics in manufacturing that can adjust operations based on real-time data to optimize production processes demonstrate agency.\\n\\n- **Personal Assistants:** AI-driven assistants like advanced versions of Siri or Alexa that can manage schedules, make reservations, and perform tasks based on user preferences exhibit agentic behavior.\\n\\n- **Autonomous Drones:** Drones used for surveillance, delivery, or environmental monitoring operate independently, making navigation and operational decisions on the fly.\\n\\n### Applications of Agentic AI\\n\\n- **Healthcare:** Autonomous diagnostic systems can analyze medical data, identify potential health issues, and recommend treatment plans without constant human supervision.\\n\\n- **Finance:** AI agents in trading can make real-time investment decisions based on market analysis, optimizing portfolios with minimal human input.\\n\\n- **Customer Service:** Advanced chatbots can handle complex customer inquiries, resolve issues, and learn from interactions to improve service quality over time.\\n\\n### Ethical and Societal Considerations\\n\\nThe rise of agentic AI brings several ethical and societal challenges:\\n\\n- **Accountability:** Determining responsibility for the actions of autonomous AI systems can be complex, especially in cases of errors or unintended consequences.\\n\\n- **Bias and Fairness:** AI agents trained on biased data can perpetuate or even exacerbate existing societal biases, leading to unfair outcomes.\\n\\n- **Privacy:** Autonomous systems that collect and process vast amounts of data may infringe on individual privacy if not properly regulated.\\n\\n- **Control and Safety:** Ensuring that agentic AI systems behave as intended and do not act in ways that could harm humans or the environment is paramount.\\n\\n### Future Directions\\n\\nAs AI technology evolves, the capabilities and applications of agentic AI are expected to expand. Future developments may include:\\n\\n- **Enhanced Learning Algorithms:** More sophisticated machine learning models that allow AI agents to understand and adapt to complex, unpredictable environments.\\n\\n- **Human-AI Collaboration:** Systems where agentic AI works alongside humans, augmenting human capabilities and enabling more effective collaboration.\\n\\n- **Regulatory Frameworks:** Establishing comprehensive policies and guidelines to govern the development and deployment of agentic AI, ensuring ethical standards and safety measures are in place.\\n\\n### Conclusion\\n\\nAgentic AI represents a significant advancement in artificial intelligence, offering the potential for more autonomous, efficient, and intelligent systems across various industries. While the benefits are substantial, it is crucial to address the associated ethical, societal, and technical challenges to ensure that agentic AI contributes positively to human progress and well-being.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1100, 'prompt_tokens': 13, 'total_tokens': 1113, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_7989eaacf6', 'id': 'chatcmpl-BanNLCoqpZFAj0XPMHDGFUfPo706V', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f23b7e14-f8fe-4ce5-baf7-b9b8f0a4112b-0' usage_metadata={'input_tokens': 13, 'output_tokens': 1100, 'total_tokens': 1113, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agentic AI** refers to artificial intelligence systems that possess a degree of agency, meaning they can act autonomously, make decisions, and pursue goals with minimal human intervention. The concept of agency in AI encapsulates the ability of a system to perceive its environment, interpret data, make informed decisions, and execute actions to achieve specific objectives. This contrasts with more passive or reactive AI systems that primarily respond to direct inputs without engaging in proactive or self-directed behavior.\n",
      "\n",
      "### Key Characteristics of Agentic AI\n",
      "\n",
      "1. **Autonomy:** Agentic AI operates independently, making decisions without continuous human oversight. This autonomy enables the system to function in dynamic environments where pre-defined instructions may not suffice.\n",
      "\n",
      "2. **Goal-Oriented Behavior:** These AI systems are designed to achieve specific objectives. They can set sub-goals, prioritize tasks, and adjust their strategies based on feedback and changing conditions.\n",
      "\n",
      "3. **Perception and Interaction:** Agentic AI can perceive its environment through various sensors or data inputs, interpret this information, and interact with the world or other systems accordingly.\n",
      "\n",
      "4. **Learning and Adaptation:** Such systems often incorporate machine learning algorithms that allow them to learn from experiences, adapt to new situations, and improve their performance over time.\n",
      "\n",
      "5. **Decision-Making Capabilities:** Agentic AI employs sophisticated algorithms to evaluate options, assess risks, and make decisions that align with their programmed goals and any ethical guidelines they might follow.\n",
      "\n",
      "### Examples of Agentic AI\n",
      "\n",
      "- **Autonomous Vehicles:** Self-driving cars are prime examples of agentic AI. They perceive their surroundings using sensors, make real-time decisions to navigate traffic, avoid obstacles, and ensure passenger safety without human intervention.\n",
      "\n",
      "- **Robotic Process Automation (RPA):** Advanced robotics in manufacturing that can adjust operations based on real-time data to optimize production processes demonstrate agency.\n",
      "\n",
      "- **Personal Assistants:** AI-driven assistants like advanced versions of Siri or Alexa that can manage schedules, make reservations, and perform tasks based on user preferences exhibit agentic behavior.\n",
      "\n",
      "- **Autonomous Drones:** Drones used for surveillance, delivery, or environmental monitoring operate independently, making navigation and operational decisions on the fly.\n",
      "\n",
      "### Applications of Agentic AI\n",
      "\n",
      "- **Healthcare:** Autonomous diagnostic systems can analyze medical data, identify potential health issues, and recommend treatment plans without constant human supervision.\n",
      "\n",
      "- **Finance:** AI agents in trading can make real-time investment decisions based on market analysis, optimizing portfolios with minimal human input.\n",
      "\n",
      "- **Customer Service:** Advanced chatbots can handle complex customer inquiries, resolve issues, and learn from interactions to improve service quality over time.\n",
      "\n",
      "### Ethical and Societal Considerations\n",
      "\n",
      "The rise of agentic AI brings several ethical and societal challenges:\n",
      "\n",
      "- **Accountability:** Determining responsibility for the actions of autonomous AI systems can be complex, especially in cases of errors or unintended consequences.\n",
      "\n",
      "- **Bias and Fairness:** AI agents trained on biased data can perpetuate or even exacerbate existing societal biases, leading to unfair outcomes.\n",
      "\n",
      "- **Privacy:** Autonomous systems that collect and process vast amounts of data may infringe on individual privacy if not properly regulated.\n",
      "\n",
      "- **Control and Safety:** Ensuring that agentic AI systems behave as intended and do not act in ways that could harm humans or the environment is paramount.\n",
      "\n",
      "### Future Directions\n",
      "\n",
      "As AI technology evolves, the capabilities and applications of agentic AI are expected to expand. Future developments may include:\n",
      "\n",
      "- **Enhanced Learning Algorithms:** More sophisticated machine learning models that allow AI agents to understand and adapt to complex, unpredictable environments.\n",
      "\n",
      "- **Human-AI Collaboration:** Systems where agentic AI works alongside humans, augmenting human capabilities and enabling more effective collaboration.\n",
      "\n",
      "- **Regulatory Frameworks:** Establishing comprehensive policies and guidelines to govern the development and deployment of agentic AI, ensuring ethical standards and safety measures are in place.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Agentic AI represents a significant advancement in artificial intelligence, offering the potential for more autonomous, efficient, and intelligent systems across various industries. While the benefits are substantial, it is crucial to address the associated ethical, societal, and technical challenges to ensure that agentic AI contributes positively to human progress and well-being.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user\\'s name is Nahid. I should start by greeting them back politely. Let me check the message again. They just introduced themselves, so the conversation is just starting.\\n\\nHmm, I need to respond in a friendly manner. Maybe say \"Hello Nahid!\" and ask how I can assist them today. Keep it open-ended so they feel comfortable to ask anything. Let me make sure there\\'s no typo and the tone is welcoming. Yep, that should work.\\n\\nWait, should I add an emoji? Maybe a smiley to keep it friendly. Okay, I\\'ll go with \"Hello Nahid! 😊 How can I assist you today?\" That sounds good. Short and to the point, but still warm.\\n</think>\\n\\nHello Nahid! 😊 How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 164, 'prompt_tokens': 16, 'total_tokens': 180, 'completion_time': 0.385252906, 'prompt_time': 0.002950339, 'queue_time': 0.224951709, 'total_time': 0.388203245}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_a91d9c2cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run--ca834f7f-0f1b-442f-b893-bc5ee0020cf4-0', usage_metadata={'input_tokens': 16, 'output_tokens': 164, 'total_tokens': 180})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the ChatGroq class from the langchain_groq integration\n",
    "# This allows you to interact with a Groq-hosted LLM (like Qwen)\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the model using the specified Groq model (e.g., Qwen 32B)\n",
    "model = ChatGroq(model=\"qwen-qwq-32b\")\n",
    "\n",
    "# Send a prompt/message to the model and receive a response\n",
    "response = model.invoke(\"Hi, my name is Nahid\")\n",
    "\n",
    "# Print or inspect the response from the model\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "# Import the ChatPromptTemplate class from langchain_core, which helps in building structured prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a prompt template with a system message and a user input placeholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # System message sets the behavior or role of the assistant\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        \n",
    "        # User message is where dynamic input will be inserted during runtime\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the prompt object (useful in notebooks to visualize the prompt structure)\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002CD52871150>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002CD52872C90>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import ChatGroq from the langchain_groq package\n",
    "# This enables access to LLMs served by Groq through LangChain\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the ChatGroq model with the specified model name\n",
    "# 'gemma2-9b-it' is the instruction-tuned version of Google's Gemma 2 9B model\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# Display the model object (in notebooks, this shows model configuration; useful for debugging)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002CD52871150>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002CD52872C90>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chaining\n",
    "\n",
    "# Chain the prompt template with the model using the pipe (|) operator.\n",
    "# This means: the output of the prompt (after filling in variables like {input}) is passed directly to the model.\n",
    "chain = prompt | model\n",
    "\n",
    "# Display the chained object. This is a Runnable object that can now be invoked with input data.\n",
    "chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a293571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I can definitely tell you about Langsmith!\n",
      "\n",
      "Langsmith is an open-source project by the Anthropic team that aims to simplify and streamline the process of building and fine-tuning large language models (LLMs). \n",
      "\n",
      "Here's a breakdown of its key features and benefits:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **User-Friendly Interface:** Langsmith provides a web-based interface that makes it easy to interact with LLMs, even for users without extensive coding experience.\n",
      "\n",
      "* **Fine-Tuning Made Easy:** It simplifies the fine-tuning process by offering pre-built templates and tools for customizing LLMs to specific tasks.\n",
      "\n",
      "* **Experiment Tracking:** Langsmith allows you to track your experiments, compare different fine-tuning strategies, and reproduce results easily.\n",
      "* **Community-Driven:** Being open-source, it fosters collaboration and encourages contributions from the wider AI community.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Accessibility:** Langsmith democratizes access to LLM development by removing the technical barriers associated with traditional fine-tuning methods.\n",
      "* **Efficiency:** Its streamlined workflow and pre-built tools significantly reduce the time and effort required to fine-tune LLMs.\n",
      "* **Transparency:** The open-source nature of Langsmith promotes transparency and allows for scrutiny and improvement by the community.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "* **Customizing LLMs for Specific Tasks:** Fine-tune a pre-trained LLM for tasks like text summarization, question answering, or code generation.\n",
      "* **Building Educational Tools:** Create interactive learning experiences and personalized tutoring systems.\n",
      "* **Research and Exploration:** Experiment with different fine-tuning techniques and explore the capabilities of LLMs.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "If you're interested in exploring Langsmith, their website provides comprehensive documentation, tutorials, and examples to get you started.\n",
      "\n",
      "**In summary, Langsmith is a powerful and user-friendly tool that empowers individuals and organizations to leverage the potential of LLMs for a wide range of applications.**\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or any other AI-related topics!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain by passing in the user input as a dictionary.\n",
    "# The key 'input' matches the placeholder {input} in the prompt template.\n",
    "response = chain.invoke({\"input\": \"Can you tell me something about Langsmith\"})\n",
    "\n",
    "# Print only the content of the response (the model's actual generated answer)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's talk about Langsmith! \n",
      "\n",
      "**Langsmith** is an open-source, community-driven platform designed to streamline the process of building, evaluating, and deploying large language models (LLMs). It's a powerful toolset that empowers developers and researchers to work with LLMs more efficiently and effectively.\n",
      "\n",
      "**Here's a breakdown of its key features and benefits:**\n",
      "\n",
      "* **Simplified LLM Development:**\n",
      "\n",
      "Langsmith provides a user-friendly interface and pre-built components that simplify the complexities of LLM development. This makes it accessible to a wider range of users, including those without extensive machine learning expertise.\n",
      "\n",
      "* **Streamlined Training and Evaluation:** The platform offers tools for efficient training, fine-tuning, and evaluating LLMs. It integrates with popular deep learning frameworks like PyTorch and TensorFlow, allowing you to leverage existing infrastructure and expertise.\n",
      "\n",
      "* **Collaborative Ecosystem:** Langsmith fosters a collaborative environment where developers can share models, datasets, and best practices. This open-source nature promotes innovation and accelerates progress in the field of LLMs.\n",
      "\n",
      "* **Model Deployment and Management:** The platform assists in deploying trained LLMs to various environments, including cloud platforms and local servers. It also provides tools for monitoring and managing deployed models, ensuring their optimal performance.\n",
      "\n",
      "* **Focus on Ethical Considerations:** Langsmith emphasizes responsible AI development by incorporating guidelines and best practices for mitigating bias, ensuring fairness, and promoting transparency in LLM development.\n",
      "\n",
      "**Who Benefits from Langsmith?**\n",
      "\n",
      "* **Researchers:**\n",
      "\n",
      "Langsmith empowers researchers to experiment with new LLM architectures, explore different training techniques, and accelerate their research endeavors.\n",
      "* **Developers:**\n",
      "\n",
      "Developers can leverage Langsmith to build LLM-powered applications, integrate LLMs into existing systems, and create innovative solutions across various domains.\n",
      "* **Data Scientists:** Langsmith provides data scientists with tools to analyze and understand the behavior of LLMs, contributing to the advancement of natural language processing research.\n",
      "\n",
      "**In Essence:**\n",
      "\n",
      "Langsmith is a comprehensive platform that democratizes access to LLM technology, making it easier for individuals and organizations to explore, develop, and deploy powerful AI solutions.\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about Langsmith or LLMs in general. I'm here to help!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "\n",
    "# Import a simple output parser that extracts the text content from the model response\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain the prompt, model, and output parser together\n",
    "# This way, the input flows through the prompt → model → output parser\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# Invoke the final chain with user input\n",
    "# The input value fills the {input} placeholder in the prompt template\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith\"})\n",
    "\n",
    "# Print the final, parsed string response from the model\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import JsonOutputParser from langchain_core\n",
    "# This parser is used to extract structured JSON outputs from language model responses\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Initialize the JSON output parser\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "# Retrieve format instructions that tell the language model how to structure its output as JSON\n",
    "output_parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "\n",
    "# Import JsonOutputParser to extract structured JSON from the LLM's output\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Import PromptTemplate to define a custom prompt structure\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the JSON output parser\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "# Create a prompt template that:\n",
    "# - Takes a user query as input\n",
    "# - Includes format instructions generated by the JSON parser to guide the LLM's output\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query:\\n{format_instruction}\\n{query}\\n\",\n",
    "    \n",
    "    # 'query' is the only input variable that will be filled at runtime\n",
    "    input_variables=[\"query\"],\n",
    "    \n",
    "    # 'format_instruction' is pre-filled using the parser's instructions\n",
    "    partial_variables={\"format_instruction\": output_parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query:\\n{format_instruction}\\n{query}\\n')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying AI applications, focused on making large language models (LLMs) accessible and usable for everyone.', 'key_features': ['User-friendly interface:', 'Modular design:', 'Fine-tuning capabilities:', 'Deployment options:', 'Community support'], 'website': 'https://www.langsmith.com/', 'github': 'https://github.com/langs-smith/langs-smith'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ee96082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed7d7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Langsmith is an open-source platform designed to simplify the process of building and deploying large language models (LLMs). \\n\\nHere are some key features of Langsmith:\\n\\n* **Model Building:** Langsmith provides tools for training and fine-tuning LLMs using various techniques like LoRA (Low-Rank Adaptation) and prompt engineering.\\n\\n* **Prompt Engineering:** It offers a user-friendly interface for creating and testing prompts, allowing developers to optimize interactions with LLMs.\\n* **Model Deployment:** Langsmith enables easy deployment of trained models as APIs, making them accessible for integration into applications.\\n* **Community-Driven:** Being open-source, Langsmith benefits from a collaborative community that contributes to its development and shares resources.\\n\\nLangsmith aims to democratize access to LLM development by providing a comprehensive and accessible platform for both beginners and experienced practitioners.'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822936",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c1c1802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6e8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "\n",
    "# Import XMLOutputParser to handle and parse structured XML outputs from the LLM\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "\n",
    "# Import PromptTemplate to build a reusable, parameterized prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the XML output parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Create a prompt template that:\n",
    "# - Instructs the model to return its answer in XML format\n",
    "# - Accepts a dynamic user query\n",
    "# - Injects format instructions automatically using partial_variables\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query:\\n{format_instruction}\\n{query}\\n\",\n",
    "    \n",
    "    # 'query' is the variable to be filled in at runtime with the user input\n",
    "    input_variables=[\"query\"],\n",
    "    \n",
    "    # 'format_instruction' is pre-filled with the required XML structure guidance\n",
    "    partial_variables={\"format_instruction\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Display the prompt object (useful for debugging or visualization in a notebook)\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "940f704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response>\\n  <name>Langsmith</name>\\n  <description>Langsmith is an open-weights AI assistant that helps developers build and deploy AI applications. </description>\\n  <features>\\n    <feature>Open-weights model</feature>\\n    <feature>Python-based framework</feature>\\n    <feature>Modular design</feature>\\n    <feature>Support for various tasks like text generation, summarization, and question answering</feature>\\n  </features>\\n  <purpose>To enable developers to easily integrate AI capabilities into their projects and build innovative applications.</purpose>\\n</response> \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 136, 'prompt_tokens': 195, 'total_tokens': 331, 'completion_time': 0.247272727, 'prompt_time': 0.009272554, 'queue_time': 0.021464904, 'total_time': 0.256545281}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--0d9b6dfe-8316-4cf8-830b-eb1492d5abd3-0' usage_metadata={'input_tokens': 195, 'output_tokens': 136, 'total_tokens': 331}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1eec50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides a collection of tools and components that streamline the process of integrating LLMs into various workflows, such as chatbots, question answering systems, and text summarization.</answer></response>\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 39, 'total_tokens': 109, 'completion_time': 0.127272727, 'prompt_time': 0.002378806, 'queue_time': 0.017950963, 'total_time': 0.129651533}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--e3bd168a-a467-4bd1-b3db-4425c7596823-0' usage_metadata={'input_tokens': 39, 'output_tokens': 70, 'total_tokens': 109}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab7431f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle stand up by itself?\",\n",
       " 'punchline': 'Because it was two tired!'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36e1dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why did the scarecrow win an award? Because he was outstanding in his field!'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f2ec0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Big</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>Philadelphia</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>The Green Mile</movie>\n",
      "<movie>Sully</movie>\n",
      "<movie>Toy Story</movie>\n",
      "<movie>Sleepless in Seattle</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c90caccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed2d4",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0831692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_name='Apple MacBook Air' product_details='A lightweight and portable laptop known for its performance, battery life, and sleek design.' tentative_price_usd=999\n"
     ]
    }
   ],
   "source": [
    "### Product Assistant with Structured Output using Pydantic and LangChain\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq  # You can use OpenAI or any other LLM too\n",
    "\n",
    "# Step 1: Define the output schema using Pydantic\n",
    "class ProductInfo(BaseModel):\n",
    "    product_name: str = Field(..., description=\"The name of the product\")\n",
    "    product_details: str = Field(..., description=\"A brief description of the product\")\n",
    "    tentative_price_usd: int = Field(..., description=\"Tentative price in USD as an integer\")\n",
    "\n",
    "# Step 2: Create the parser from the schema\n",
    "output_parser = PydanticOutputParser(pydantic_object=ProductInfo)\n",
    "\n",
    "# Step 3: Create the prompt with format instructions\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that provides product information.\"),\n",
    "    (\"user\", \"{input}\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "# Step 4: Convert the parser's format instructions into a partial variable for the prompt\n",
    "prompt = prompt.partial(format_instructions=output_parser.get_format_instructions())\n",
    "\n",
    "# Step 5: Initialize your model (swap with your preferred one if needed)\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# Step 6: Chain the prompt → model → output parser\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# Step 7: Run the assistant with a query\n",
    "response = chain.invoke({\"input\": \"Tell me about the Apple MacBook Air\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2999f98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
